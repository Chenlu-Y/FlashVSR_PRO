# CUDA显存溢出（OOM）错误解决方案

## 错误分析

### 错误信息解读

```
CUDA out of memory. Tried to allocate 144.00 MiB. 
GPU 1 has a total capacity of 31.48 GiB of which 88.31 MiB is free.
Process 2033842 has 21.70 GiB memory in use.
Process 1945295 has 9.67 GiB memory in use.
```

**含义**：
- GPU 1 总容量：31.48 GiB
- 空闲显存：仅 88.31 MiB（几乎耗尽）
- 两个进程占用显存：
  - Process 2033842: 21.70 GiB
  - Process 1945295: 9.67 GiB
- 尝试分配：144 MiB（失败）

### 问题原因

1. **多个进程占用同一GPU**
   - 可能多个worker被分配到GPU 1
   - 或其他进程占用了显存

2. **显存未正确释放**
   - 之前的进程没有完全释放显存
   - PyTorch显存碎片化

3. **显存分配冲突**
   - 多GPU处理时，worker进程可能冲突

## 解决方案

### 方案1: 清理占用显存的进程（推荐）

```bash
# 1. 查看GPU使用情况
nvidia-smi

# 2. 查看具体进程
nvidia-smi pmon -c 1

# 3. 如果发现不需要的进程，终止它们
# 注意：确保这些进程不是重要的任务
kill -9 2033842  # 谨慎使用，确保进程不重要
kill -9 1945295  # 谨慎使用，确保进程不重要

# 4. 清理PyTorch缓存
python -c "import torch; torch.cuda.empty_cache(); print('Cache cleared')"
```

### 方案2: 使用单GPU模式（临时解决）

如果多GPU有问题，先使用单GPU：

```bash
# 不使用 --multi_gpu 参数
python infer_video.py \
  --input /app/input/video.mp4 \
  --output /app/output/output.mp4 \
  --mode tiny \
  --scale 4 \
  --tiled_dit True \
  --device cuda:0  # 指定使用GPU 0
```

### 方案3: 降低显存占用

```bash
# 使用更保守的参数
python infer_video.py \
  --input /app/input/video.mp4 \
  --output /app/output/output.mp4 \
  --mode tiny-long \
  --scale 2 \
  --tiled_dit True \
  --tile_size 128 \
  --tile_overlap 16 \
  --unload_dit True \
  --kv_ratio 1.0
```

### 方案4: 设置PyTorch显存管理环境变量

```bash
# 在运行前设置
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# 然后运行
python infer_video.py ...
```

或在Docker中：
```bash
docker exec -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  flashvsr_ultra_fast python /app/FlashVSR_Ultra_Fast/infer_video.py ...
```

### 方案5: 检查多GPU分配逻辑

如果使用多GPU，确保每个GPU只处理一个segment：

```bash
# 检查GPU数量
nvidia-smi --list-gpus

# 如果只有2个GPU，确保只启动2个worker
# 代码会自动处理，但如果有其他进程占用，需要先清理
```

### 方案6: 重启Docker容器（彻底清理）

```bash
# 停止容器
docker-compose down

# 清理显存
nvidia-smi --gpu-reset  # 如果支持

# 重新启动
docker-compose up -d

# 验证GPU状态
docker exec flashvsr_ultra_fast nvidia-smi
```

## 预防措施

### 1. 运行前检查GPU状态

```bash
# 检查GPU使用情况
nvidia-smi

# 确保每个GPU有足够空闲显存（建议至少10GB）
```

### 2. 使用显存监控

```bash
# 实时监控GPU使用
watch -n 1 nvidia-smi
```

### 3. 合理设置参数

- **显存 < 12GB**: 使用 `--mode tiny-long --scale 2 --tile_size 128`
- **显存 12-16GB**: 使用 `--mode tiny --scale 4 --tile_size 256`
- **显存 16-24GB**: 可以启用 `--adaptive_batch_size`
- **显存 > 24GB**: 可以使用 `--mode full`

### 4. 避免同时运行多个任务

- 一次只运行一个超分任务
- 如果必须多任务，确保分配到不同GPU

## 常见问题

### Q: 为什么多GPU时会出现显存冲突？

A: 多GPU处理时，每个worker进程会在指定GPU上加载完整模型。如果：
- 多个worker被分配到同一GPU
- 有其他进程占用显存
- 显存没有正确释放

就会导致显存不足。

### Q: 如何确保每个GPU只处理一个任务？

A: 代码会自动分配，但需要确保：
1. 没有其他进程占用GPU
2. 每个GPU有足够显存（建议至少12GB）
3. 正确设置 `NVIDIA_VISIBLE_DEVICES`

### Q: 显存碎片化怎么办？

A: 设置环境变量：
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

## 快速诊断命令

```bash
# 1. 查看GPU状态
nvidia-smi

# 2. 查看进程占用
nvidia-smi pmon -c 1

# 3. 查看PyTorch显存使用
python -c "import torch; print(f'Allocated: {torch.cuda.memory_allocated()/1024**3:.2f}GB'); print(f'Reserved: {torch.cuda.memory_reserved()/1024**3:.2f}GB')"

# 4. 清理显存
python -c "import torch; torch.cuda.empty_cache()"
```

